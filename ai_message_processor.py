import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
import requests  # ç”¨äºè°ƒç”¨OpenRouter API
import base64
import os
from dataclasses import dataclass
from feishu_user_id_mapper import get_user_name_by_feishu_id, replace_user_ids_in_text


@dataclass
class TaskItem:
    """å•ä¸ªä»»åŠ¡é¡¹"""
    id: str
    title: str
    description: str
    assignee: str  # è´Ÿè´£äºº
    priority: str  # HIGH, MEDIUM, LOW
    status: str  # TODO, DONE, ISSUE
    tags: List[str]
    deadline: Optional[str] = None
    related_messages: List[str] = None  # ç›¸å…³çš„æ¶ˆæ¯ID
    confidence: float = 0.0


@dataclass
class PersonTasks:
    """ä¸ªäººä»»åŠ¡æ±‡æ€»"""
    person_id: str
    person_name: str
    todos: List[TaskItem]
    dones: List[TaskItem] 
    issues: List[TaskItem]


class AIProjectAnalyzer:
    def __init__(self, api_key: str = None, model: str = "google/gemini-2.5-pro-preview", api_url: str = "https://openrouter.ai/api/v1/chat/completions"):
        """
        åˆå§‹åŒ–AIé¡¹ç›®åˆ†æå™¨
        
        Args:
            api_key: OpenRouter APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆOpenRouteræ ¼å¼ï¼‰
            api_url: OpenRouter APIåœ°å€
        """
        self.api_key = api_key
        self.model = model
        self.api_url = api_url
    
    def analyze_project_context(self, messages_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææ•´ä¸ªé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œæå–ä»»åŠ¡å¹¶æŒ‰äººå‘˜åˆ†ç»„
        
        Args:
            messages_data: ä»é£ä¹¦è·å–çš„æ¶ˆæ¯æ•°æ®
            
        Returns:
            æŒ‰äººå‘˜åˆ†ç»„çš„ä»»åŠ¡åˆ†æç»“æœ
        """
        print("ğŸ” å¼€å§‹é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æ...")
        
        try:
            messages = messages_data.get("messages", [])
            if not messages:
                return self._create_empty_result()
            
            # é¢„å¤„ç†æ¶ˆæ¯ï¼Œæ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            print("ğŸ“Š æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡...")
            context_data = self._build_project_context_with_documents(messages)
            
            # ä½¿ç”¨AIè¿›è¡Œæ·±åº¦åˆ†æ
            print("ğŸ¤– å¼€å§‹AIä»»åŠ¡åˆ†æ...")
            if not self.api_key:
                raise ValueError("éœ€è¦æä¾›OpenRouter APIå¯†é’¥æ‰èƒ½è¿›è¡Œåˆ†æ")
                
            analysis_result = self._analyze_with_openrouter_gemini(context_data)
            
            # åå¤„ç†å’Œç»Ÿè®¡
            print("ğŸ“‹ ç»„ç»‡åˆ†æç»“æœ...")
            final_result = self._organize_by_person(analysis_result, messages_data)
            
            print(f"âœ… é¡¹ç›®åˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len(analysis_result)} ä¸ªä»»åŠ¡é¡¹")
            return final_result
            
        except Exception as e:
            print(f"âŒ é¡¹ç›®åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise e
    
    def _build_project_context_with_documents(self, messages: List[Dict]) -> Dict[str, Any]:
        """æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œæš‚æ—¶è·³è¿‡æ–‡æ¡£å¤„ç†ä»¥æé«˜é€Ÿåº¦"""
        print("ğŸ“Š æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆä»…æ–‡æœ¬æ¶ˆæ¯ï¼‰...")
        
        # æå–äººå‘˜ä¿¡æ¯
        participants = {}
        
        # æŒ‰æ—¶é—´æ’åºçš„æ¶ˆæ¯
        sorted_messages = sorted(messages, key=lambda x: x.get("create_time", 0))
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        conversation_flow = []
        
        for msg in sorted_messages:
            msg_type = msg.get("msg_type", "")
            sender_info = msg.get("sender", {})
            sender_id = sender_info.get("id", "unknown")
            
            # æ”¶é›†å‚ä¸è€…ä¿¡æ¯
            if sender_id not in participants:
                participants[sender_id] = {
                    "id": sender_id,
                    "name": self._extract_person_name(sender_info),
                    "message_count": 0
                }
            participants[sender_id]["message_count"] += 1
            
            # åªå¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼Œè·³è¿‡æ–‡ä»¶æ¶ˆæ¯ä»¥æé«˜é€Ÿåº¦
            if msg_type == "text":
                text = msg.get("text", "").strip()
                if text and len(text) >= 2:
                    # åœ¨æ–‡æœ¬å†…å®¹ä¸­ä¹Ÿæ›¿æ¢ç”¨æˆ·IDä¸ºçœŸå®å§“å
                    text = replace_user_ids_in_text(text)
                    conversation_item = {
                        "message_id": msg.get("message_id"),
                        "timestamp": msg.get("create_time"),
                        "sender": participants[sender_id],
                        "content": text,
                        "type": "text",
                        "mentions": msg.get("mentions", []),
                        "original_msg": msg
                    }
                    conversation_flow.append(conversation_item)
            
            # å¯¹äºæ–‡ä»¶æ¶ˆæ¯ï¼Œåªè®°å½•æ–‡ä»¶åï¼Œä¸è¯»å–å†…å®¹
            elif msg_type == "file":
                files = msg.get("files", [])
                for file_info in files:
                    file_name = file_info.get('file_name', 'æœªçŸ¥æ–‡æ¡£')
                    conversation_item = {
                        "message_id": msg.get("message_id"),
                        "timestamp": msg.get("create_time"),
                        "sender": participants[sender_id],
                        "content": f"[æ–‡æ¡£] {file_name}",
                        "type": "file_reference",
                        "file_info": file_info,
                        "original_msg": msg
                    }
                    conversation_flow.append(conversation_item)
        
        return {
            "participants": participants,
            "conversation_flow": conversation_flow,
            "document_contents": [],  # æš‚æ—¶ä¸ºç©ºï¼Œä¸å¤„ç†æ–‡æ¡£å†…å®¹
            "total_messages": len(conversation_flow),
            "total_documents": 0,  # æš‚æ—¶è®¾ä¸º0
            "timespan": {
                "start": conversation_flow[0]["timestamp"] if conversation_flow else None,
                "end": conversation_flow[-1]["timestamp"] if conversation_flow else None
            }
        }
    
    def _extract_document_content(self, file_info: Dict) -> str:
        """æå–æ–‡æ¡£å†…å®¹"""
        try:
            file_path = file_info.get("file_path", "")
            file_name = file_info.get("file_name", "")
            
            if not file_path or not os.path.exists(file_path):
                return f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„æ— æ•ˆ: {file_name}"
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åå¤„ç†ä¸åŒç±»å‹çš„æ–‡æ¡£
            file_ext = os.path.splitext(file_name.lower())[1]
            
            if file_ext in ['.txt', '.md', '.log']:
                # çº¯æ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content[:5000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å…è¿‡é•¿
            
            elif file_ext in ['.pdf']:
                # PDFæ–‡ä»¶ï¼ˆéœ€è¦å®‰è£…pdfplumberæˆ–ç±»ä¼¼åº“ï¼‰
                try:
                    import pdfplumber
                    with pdfplumber.open(file_path) as pdf:
                        text = ""
                        for page in pdf.pages[:10]:  # é™åˆ¶é¡µæ•°
                            text += page.extract_text() or ""
                    return text[:5000]
                except ImportError:
                    return f"PDFæ–‡æ¡£éœ€è¦å®‰è£…pdfplumberåº“æ‰èƒ½è¯»å–: {file_name}"
                except Exception as e:
                    return f"æ— æ³•è¯»å–PDFæ–‡æ¡£: {file_name}, é”™è¯¯: {str(e)}"
            
            elif file_ext in ['.docx']:
                # Wordæ–‡æ¡£ï¼ˆéœ€è¦å®‰è£…python-docxï¼‰
                try:
                    from docx import Document
                    doc = Document(file_path)
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text[:5000]
                except ImportError:
                    return f"Wordæ–‡æ¡£éœ€è¦å®‰è£…python-docxåº“æ‰èƒ½è¯»å–: {file_name}"
                except Exception as e:
                    return f"æ— æ³•è¯»å–Wordæ–‡æ¡£: {file_name}, é”™è¯¯: {str(e)}"
            
            else:
                return f"æš‚ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {file_name} ({file_ext})"
                
        except Exception as e:
            return f"è¯»å–æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _analyze_with_openrouter_gemini(self, context_data: Dict[str, Any]) -> List[TaskItem]:
        """ä½¿ç”¨OpenRouter Gemini 2.5è¿›è¡Œæ·±åº¦é¡¹ç›®åˆ†æ"""
        print("ğŸ¤– ä½¿ç”¨OpenRouter Gemini 2.5è¿›è¡Œé¡¹ç›®åˆ†æ...")
        
        # æ„å»ºAIåˆ†æprompt
        prompt = self._build_comprehensive_analysis_prompt_with_documents(context_data)
        
        headers = {
            "Content-Type": "application/json; charset=utf-8",
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://feishu-analyzer.com",
            "X-Title": "Feishu Message Analyzer"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system", 
                    "content": """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„é¡¹ç›®ç®¡ç†ä¸“å®¶å’Œå¼€å‘å›¢é˜Ÿé¡¾é—®ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå¼€å‘å›¢é˜Ÿçš„ç¾¤èŠè®°å½•ï¼Œä»ä¸­æå–å‡ºçœŸæ­£çš„é¡¹ç›®ä»»åŠ¡ã€å·²å®Œæˆçš„å·¥ä½œå’Œé‡åˆ°çš„é—®é¢˜ã€‚

ä½ éœ€è¦ï¼š
1. æ·±åº¦ç†è§£æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡å’Œé¡¹ç›®èƒŒæ™¯
2. è¯†åˆ«å‡ºå…·ä½“çš„å¼€å‘ä»»åŠ¡ã€åŠŸèƒ½éœ€æ±‚ã€bugä¿®å¤ç­‰
3. è·Ÿè¸ªä»»åŠ¡çš„çŠ¶æ€å˜åŒ–ï¼ˆè°è´Ÿè´£ã€æ˜¯å¦å®Œæˆã€é‡åˆ°ä»€ä¹ˆé—®é¢˜ï¼‰
4. è¯†åˆ«ä»»åŠ¡çš„ä¼˜å…ˆçº§å’Œæˆªæ­¢æ—¶é—´
5. æŒ‰ç…§å›¢é˜Ÿæˆå‘˜ç»„ç»‡ä»»åŠ¡åˆ†é…
6. æå–æŠ€æœ¯æ ‡ç­¾å’Œå…³é”®ä¿¡æ¯

é‡è¦ï¼šä¸è¦ç®€å•åœ°å¯¹æ¯æ¡æ¶ˆæ¯åˆ†ç±»ï¼Œè€Œæ˜¯è¦ç†è§£æ•´ä¸ªé¡¹ç›®çš„ä»»åŠ¡æµç¨‹å’Œå›¢é˜Ÿåä½œæ¨¡å¼ã€‚"""
                },
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.2,
            "max_tokens": 1500
        }
        
        response = requests.post(self.api_url, headers=headers, json=payload)
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            print("âœ… OpenRouter Gemini 2.5åˆ†æå®Œæˆ")
            return self._parse_ai_analysis(ai_response, context_data)
        else:
            error_msg = f"OpenRouter APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {response.text}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
    
    def _build_comprehensive_analysis_prompt_with_documents(self, context_data: Dict[str, Any]) -> str:
        """æ„å»ºåˆ†æpromptï¼Œä¸“æ³¨äºæ–‡æœ¬æ¶ˆæ¯åˆ†æ"""
        participants = context_data["participants"]
        conversation = context_data["conversation_flow"]
        
        # æ„å»ºå‚ä¸è€…åˆ—è¡¨
        participants_text = "é¡¹ç›®å›¢é˜Ÿæˆå‘˜ï¼š\n"
        for p_id, p_info in participants.items():
            participants_text += f"- {p_info['name']} (ID: {p_id}, æ¶ˆæ¯æ•°: {p_info['message_count']})\n"
        
        # æ„å»ºå®Œæ•´å¯¹è¯å†…å®¹
        conversation_text = "\n## å®Œæ•´ç¾¤èŠå¯¹è¯è®°å½•ï¼š\n"
        for i, msg in enumerate(conversation):
            sender_name = msg["sender"]["name"]
            content = msg["content"]
            msg_type = msg.get("type", "text")
            
            # å®‰å…¨åœ°å¤„ç†æ—¶é—´æˆ³è½¬æ¢
            try:
                timestamp = datetime.fromtimestamp(int(msg["timestamp"])).strftime("%m-%d %H:%M")
            except (ValueError, OSError, OverflowError):
                # å¦‚æœæ—¶é—´æˆ³æœ‰é—®é¢˜ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºæ ‡è¯†
                timestamp = f"æ¶ˆæ¯{i+1}"
            
            if msg_type == "file_reference":
                conversation_text += f"[{timestamp}] {sender_name} åˆ†äº«äº†æ–‡æ¡£: {content}\n"
            else:
                conversation_text += f"[{timestamp}] {sender_name}: {content}\n"
            
            # å¦‚æœæœ‰@æåˆ°çš„äººï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
            if msg.get("mentions"):
                mentions = [m.get("name", m.get("id", "")) for m in msg["mentions"]]
                conversation_text += f"    (æåˆ°: {', '.join(mentions)})\n"
        
        # æ—¶é—´èŒƒå›´ - å®‰å…¨å¤„ç†
        timespan = context_data["timespan"]
        try:
            start_time = datetime.fromtimestamp(int(timespan["start"])).strftime("%Y-%m-%d %H:%M:%S")
            end_time = datetime.fromtimestamp(int(timespan["end"])).strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, OSError, OverflowError):
            start_time = "å¼€å§‹æ—¶é—´"
            end_time = "ç»“æŸæ—¶é—´"
        
        prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹å¼€å‘å›¢é˜Ÿçš„é¡¹ç›®ç¾¤èŠå¯¹è¯ï¼Œæå–å‡ºçœŸæ­£çš„ä»»åŠ¡ä¿¡æ¯å¹¶æŒ‰äººå‘˜ç»„ç»‡ï¼š

## åŸºæœ¬ä¿¡æ¯
- æ—¶é—´èŒƒå›´: {start_time} è‡³ {end_time}
- æ¶ˆæ¯æ€»æ•°: {context_data["total_messages"]}æ¡
- å‚ä¸äººæ•°: {len(participants)}äºº

{participants_text}

{conversation_text}

## åˆ†æè¦æ±‚

è¯·ä»è¿™äº›å¯¹è¯ä¸­æ·±åº¦æŒ–æ˜å¹¶è¯†åˆ«å‡ºï¼š

### 1. TODOä»»åŠ¡ (å¾…åŠäº‹é¡¹)
- éœ€è¦å®Œæˆçš„å¼€å‘ä»»åŠ¡ã€åŠŸèƒ½éœ€æ±‚ã€ä¿®å¤å·¥ä½œç­‰
- è¢«æ˜ç¡®åˆ†é…ç»™æŸäººçš„å·¥ä½œ
- è®¡åˆ’ä¸­çš„å¼€å‘å†…å®¹

### 2. DONEä»»åŠ¡ (å·²å®Œæˆ)
- å·²ç»å®Œæˆçš„å·¥ä½œã€ä¸Šçº¿çš„åŠŸèƒ½ã€è§£å†³çš„é—®é¢˜ç­‰
- å®Œæˆçš„å¼€å‘ä»»åŠ¡å’Œé‡Œç¨‹ç¢‘
- å·²äº¤ä»˜çš„æˆæœ

### 3. ISSUEé—®é¢˜ (æŠ€æœ¯é—®é¢˜)
- é‡åˆ°çš„æŠ€æœ¯é—®é¢˜ã€bugã€ç³»ç»Ÿæ•…éšœç­‰
- å¼€å‘è¿‡ç¨‹ä¸­çš„é˜»å¡å’Œå›°éš¾
- éœ€è¦è§£å†³çš„æŠ€æœ¯éš¾é¢˜

## è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«è¯¦ç»†çš„ä»»åŠ¡ä¿¡æ¯ï¼š

```json
{{
  "analysis_summary": {{
    "total_tasks_identified": 0,
    "messages_analyzed": {context_data["total_messages"]},
    "analysis_timestamp": "{datetime.now().isoformat()}"
  }},
  "tasks": [
    {{
      "id": "task_001",
      "title": "å®ç°ç”¨æˆ·ç™»å½•åŠŸèƒ½",
      "description": "éœ€è¦å¼€å‘ç”¨æˆ·ç™»å½•é¡µé¢å’Œåç«¯APIæ¥å£ï¼ŒåŒ…æ‹¬å‰ç«¯ç•Œé¢è®¾è®¡å’Œåç«¯è®¤è¯é€»è¾‘",
      "assignee": "å¼ ä¸‰",
      "assignee_id": "ou_zhang",
      "status": "TODO",
      "priority": "HIGH",
      "tags": ["frontend", "backend", "authentication"],
      "deadline": "2025-06-05",
      "source_type": "conversation",
      "related_messages": ["msg_001", "msg_002"],
      "confidence": 0.9
    }}
  ],
  "team_summary": {{
    "total_members": {len(participants)},
    "task_distribution": {{
      "å¼ ä¸‰": {{"todo": 2, "done": 1, "issue": 0}},
      "æå››": {{"todo": 1, "done": 2, "issue": 1}}
    }}
  }}
}}
```

## åˆ†æé‡ç‚¹

1. **ä»»åŠ¡å…³è”æ€§**: è¯†åˆ«åŒä¸€ä¸ªä»»åŠ¡åœ¨ä¸åŒæ¶ˆæ¯ä¸­çš„æåŠ
2. **äººå‘˜åˆ†å·¥**: å‡†ç¡®è¯†åˆ«è°è´Ÿè´£ä»€ä¹ˆä»»åŠ¡
3. **ä¼˜å…ˆçº§åˆ¤æ–­**: æ ¹æ®"ç´§æ€¥"ã€"é‡è¦"ã€"asap"ç­‰å…³é”®è¯åˆ¤æ–­
4. **æŠ€æœ¯åˆ†ç±»**: æ ¹æ®æŠ€æœ¯å…³é”®è¯è¿›è¡Œæ ‡ç­¾åˆ†ç±»
5. **çŠ¶æ€è·Ÿè¸ª**: è·Ÿè¸ªä»»åŠ¡ä»æå‡ºåˆ°å®Œæˆçš„çŠ¶æ€å˜åŒ–

è¯·ç¡®ä¿æå–çš„æ˜¯çœŸæ­£çš„å·¥ä½œä»»åŠ¡ï¼Œè€Œä¸æ˜¯ç®€å•çš„èŠå¤©å†…å®¹æˆ–è®¨è®ºã€‚é‡ç‚¹å…³æ³¨é¡¹ç›®å¼€å‘ç›¸å…³çš„å…·ä½“å·¥ä½œå†…å®¹ã€‚
"""
        return prompt
    
    def _parse_ai_analysis(self, ai_response: str, context_data: Dict) -> List[TaskItem]:
        """è§£æAIåˆ†æç»“æœ"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_response, re.DOTALL)
            if json_match:
                ai_data = json.loads(json_match.group(1))
            else:
                # å°è¯•ç›´æ¥è§£æ
                json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
                if json_match:
                    ai_data = json.loads(json_match.group())
                else:
                    ai_data = json.loads(ai_response)
            
            tasks = []
            # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ ¼å¼
            if 'tasks' in ai_data:
                task_list = ai_data['tasks']
            else:
                task_list = ai_data if isinstance(ai_data, list) else []
            
            for item in task_list:
                task = TaskItem(
                    id=item.get("id", f"ai_task_{len(tasks)+1}"),
                    title=item.get("title", "æœªçŸ¥ä»»åŠ¡"),
                    description=item.get("description", ""),
                    assignee=item.get("assignee", "æœªæŒ‡å®š"),
                    priority=item.get("priority", "LOW"),
                    status=item.get("status", "TODO"),
                    tags=item.get("tags", []),
                    deadline=item.get("deadline"),
                    related_messages=item.get("related_messages", []),
                    confidence=item.get("confidence", 0.7)
                )
                tasks.append(task)
            
            return tasks
            
        except Exception as e:
            print(f"âš ï¸ è§£æAIåˆ†æç»“æœå¤±è´¥: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å›é€€åˆ°è§„åˆ™å¼•æ“
            return []
    
    def _extract_person_name(self, sender_info: Dict) -> str:
        """æå–äººå‘˜å§“å"""
        sender_id = sender_info.get("id", "")
        if sender_id:
            # ä½¿ç”¨é£ä¹¦ç”¨æˆ·IDæ˜ å°„è·å–çœŸå®å§“å
            real_name = get_user_name_by_feishu_id(sender_id)
            if not real_name.startswith("ç”¨æˆ·") and real_name != "æœªçŸ¥ç”¨æˆ·":
                return real_name
        
        # å¦‚æœæ˜ å°„å¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„é€»è¾‘
        return sender_info.get("name", sender_info.get("id", "æœªçŸ¥ç”¨æˆ·"))
    
    def _organize_by_person(self, tasks: List[TaskItem], original_data: Dict) -> Dict[str, Any]:
        """æŒ‰äººå‘˜ç»„ç»‡ä»»åŠ¡"""
        # æŒ‰äººå‘˜åˆ†ç»„
        person_tasks = {}
        
        for task in tasks:
            assignee = task.assignee
            if assignee not in person_tasks:
                person_tasks[assignee] = {
                    "person_name": assignee,
                    "todos": [],
                    "dones": [],
                    "issues": []
                }
            
            task_dict = {
                "id": task.id,
                "title": task.title,
                "description": task.description,
                "priority": task.priority,
                "tags": task.tags,
                "deadline": task.deadline,
                "related_messages": task.related_messages,
                "confidence": task.confidence
            }
            
            if task.status == "TODO":
                person_tasks[assignee]["todos"].append(task_dict)
            elif task.status == "DONE":
                person_tasks[assignee]["dones"].append(task_dict)
            elif task.status == "ISSUE":
                person_tasks[assignee]["issues"].append(task_dict)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_todos = sum(len(p["todos"]) for p in person_tasks.values())
        total_dones = sum(len(p["dones"]) for p in person_tasks.values())
        total_issues = sum(len(p["issues"]) for p in person_tasks.values())
        
        return {
            "success": True,
            "summary": {
                "analysis_type": "project_context_analysis",
                "original_data": {
                    "total_messages": original_data.get("total_count", 0),
                    "time_range": original_data.get("time_range", {})
                },
                "task_statistics": {
                    "total_tasks": len(tasks),
                    "total_todos": total_todos,
                    "total_dones": total_dones,
                    "total_issues": total_issues,
                    "total_assignees": len(person_tasks),
                    "high_priority_tasks": sum(1 for t in tasks if t.priority == "HIGH")
                },
                "analyzed_at": datetime.now().isoformat()
            },
            "team_tasks": person_tasks,
            "metadata": {
                "analyzer_version": "2.0.0",
                "ai_model": self.model,
                "processing_mode": "AI" if self.api_key else "Enhanced Rules"
            }
        }
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            "success": False,
            "message": "æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ¶ˆæ¯å†…å®¹",
            "team_tasks": {},
            "summary": {
                "task_statistics": {
                    "total_tasks": 0,
                    "total_assignees": 0
                }
            }
        }
    
    def save_analysis_result(self, result: Dict[str, Any], output_file: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"project_analysis_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=4)
        
        print(f"ğŸ“„ é¡¹ç›®åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file


# ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„ç±»åä½œä¸ºåˆ«å
AIMessageProcessor = AIProjectAnalyzer 