import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
import requests  # ç”¨äºè°ƒç”¨OpenRouter API
import base64
import os
from dataclasses import dataclass
from feishu_user_id_mapper import get_user_name_by_feishu_id, replace_user_ids_in_text


@dataclass
class TaskItem:
    """å•ä¸ªä»»åŠ¡é¡¹"""
    id: str
    title: str
    description: str
    assignee: str  # è´Ÿè´£äºº
    priority: str  # HIGH, MEDIUM, LOW
    status: str  # TODO, DONE, ISSUE
    tags: List[str]
    deadline: Optional[str] = None
    related_messages: List[str] = None  # ç›¸å…³çš„æ¶ˆæ¯ID
    confidence: float = 0.0


@dataclass
class PersonTasks:
    """ä¸ªäººä»»åŠ¡æ±‡æ€»"""
    person_id: str
    person_name: str
    todos: List[TaskItem]
    dones: List[TaskItem] 
    issues: List[TaskItem]


class AIProjectAnalyzer:
    def __init__(self, api_key: str = None, model: str = "deepseek-chat", api_url: str = "https://api.deepseek.com/v1/chat/completions"):
        """
        åˆå§‹åŒ–AIé¡¹ç›®åˆ†æå™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆDeepSeekæ ¼å¼ï¼‰
            api_url: DeepSeek APIåœ°å€
        """
        self.api_key = api_key
        self.model = model
        self.api_url = api_url
    
    def analyze_project_context(self, messages_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¯¹é£ä¹¦ç¾¤èŠæ¶ˆæ¯è¿›è¡Œæ·±åº¦é¡¹ç›®åˆ†æ
        
        Args:
            messages_data: åŒ…å«æ¶ˆæ¯åˆ—è¡¨çš„å­—å…¸
            
        Returns:
            åŒ…å«æŒ‰äººå‘˜åˆ†ç»„çš„ä»»åŠ¡ä¿¡æ¯çš„å­—å…¸
        """
        print("ğŸ” å¼€å§‹é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æ...")
        
        try:
            messages = messages_data.get("messages", [])
            if not messages:
                return self._create_empty_result()
            
            # é¢„å¤„ç†æ¶ˆæ¯ï¼Œæ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            print("ğŸ“Š æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡...")
            context_data = self._build_project_context_with_documents(messages)
            
            # ä½¿ç”¨AIè¿›è¡Œæ·±åº¦åˆ†æ
            print("ğŸ¤– å¼€å§‹AIä»»åŠ¡åˆ†æ...")
            if not self.api_key:
                raise ValueError("éœ€è¦æä¾›DeepSeek APIå¯†é’¥æ‰èƒ½è¿›è¡Œåˆ†æ")
                
            analysis_result = self._analyze_with_openrouter_deepseek(context_data)
            
            # æ£€æŸ¥AIæ˜¯å¦ç›´æ¥è¿”å›äº†åˆ†ç»„æ•°æ®
            if isinstance(analysis_result, dict) and 'ToDo' in analysis_result:
                print("ğŸ“‹ ä½¿ç”¨AIç›´æ¥è¿”å›çš„åˆ†ç»„æ ¼å¼...")
                # ç›´æ¥ä½¿ç”¨AIè¿”å›çš„åˆ†ç»„æ•°æ®
                final_result = {
                    "success": True,
                    "daily_todolist": analysis_result,  # ä½¿ç”¨AIç›´æ¥è¿”å›çš„æ ¼å¼
                    "analysis_info": {
                        "analysis_date": datetime.now().strftime("%Y-%m-%d"),
                        "analysis_timestamp": datetime.now().isoformat(),
                        "container_id": messages_data.get("container_id", "unknown"),
                        "total_messages": len(messages),
                        "ai_model": self.model
                    },
                    "message_count": len(messages),
                    "status": "success"
                }
            else:
                # å›é€€åˆ°åŸæœ‰çš„TaskItemå¤„ç†æ–¹å¼
                print("ğŸ“‹ ä½¿ç”¨TaskItemæ ¼å¼å¤„ç†...")
                final_result = self._organize_by_person(analysis_result, messages_data)
            
            print(f"âœ… é¡¹ç›®åˆ†æå®Œæˆ")
            return final_result
            
        except Exception as e:
            print(f"âŒ é¡¹ç›®åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise e
    
    def _build_project_context_with_documents(self, messages: List[Dict]) -> Dict[str, Any]:
        """æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œæš‚æ—¶è·³è¿‡æ–‡æ¡£å¤„ç†ä»¥æé«˜é€Ÿåº¦"""
        print("ğŸ“Š æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆä»…æ–‡æœ¬æ¶ˆæ¯ï¼‰...")
        
        # æå–äººå‘˜ä¿¡æ¯
        participants = {}
        
        # æŒ‰æ—¶é—´æ’åºçš„æ¶ˆæ¯
        sorted_messages = sorted(messages, key=lambda x: x.get("create_time", 0))
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        conversation_flow = []
        
        for msg in sorted_messages:
            msg_type = msg.get("msg_type", "")
            sender_info = msg.get("sender", {})
            sender_id = sender_info.get("id", "unknown")
            
            # æ”¶é›†å‚ä¸è€…ä¿¡æ¯
            if sender_id not in participants:
                participants[sender_id] = {
                    "id": sender_id,
                    "name": self._extract_person_name(sender_info),
                    "message_count": 0
                }
            participants[sender_id]["message_count"] += 1
            
            # åªå¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼Œè·³è¿‡æ–‡ä»¶æ¶ˆæ¯ä»¥æé«˜é€Ÿåº¦
            if msg_type == "text":
                text = msg.get("text", "").strip()
                if text and len(text) >= 2:
                    # åœ¨æ–‡æœ¬å†…å®¹ä¸­ä¹Ÿæ›¿æ¢ç”¨æˆ·IDä¸ºçœŸå®å§“å
                    text = replace_user_ids_in_text(text)
                    conversation_item = {
                        "message_id": msg.get("message_id"),
                        "timestamp": msg.get("create_time"),
                        "sender": participants[sender_id],
                        "content": text,
                        "type": "text",
                        "mentions": msg.get("mentions", []),
                        "original_msg": msg
                    }
                    conversation_flow.append(conversation_item)
            
            # å¯¹äºæ–‡ä»¶æ¶ˆæ¯ï¼Œåªè®°å½•æ–‡ä»¶åï¼Œä¸è¯»å–å†…å®¹
            elif msg_type == "file":
                files = msg.get("files", [])
                for file_info in files:
                    file_name = file_info.get('file_name', 'æœªçŸ¥æ–‡æ¡£')
                    conversation_item = {
                        "message_id": msg.get("message_id"),
                        "timestamp": msg.get("create_time"),
                        "sender": participants[sender_id],
                        "content": f"[æ–‡æ¡£] {file_name}",
                        "type": "file_reference",
                        "file_info": file_info,
                        "original_msg": msg
                    }
                    conversation_flow.append(conversation_item)
        
        return {
            "participants": participants,
            "conversation_flow": conversation_flow,
            "document_contents": [],  # æš‚æ—¶ä¸ºç©ºï¼Œä¸å¤„ç†æ–‡æ¡£å†…å®¹
            "total_messages": len(conversation_flow),
            "total_documents": 0,  # æš‚æ—¶è®¾ä¸º0
            "timespan": {
                "start": conversation_flow[0]["timestamp"] if conversation_flow else None,
                "end": conversation_flow[-1]["timestamp"] if conversation_flow else None
            }
        }
    
    def _extract_document_content(self, file_info: Dict) -> str:
        """æå–æ–‡æ¡£å†…å®¹"""
        try:
            file_path = file_info.get("file_path", "")
            file_name = file_info.get("file_name", "")
            
            if not file_path or not os.path.exists(file_path):
                return f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„æ— æ•ˆ: {file_name}"
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åå¤„ç†ä¸åŒç±»å‹çš„æ–‡æ¡£
            file_ext = os.path.splitext(file_name.lower())[1]
            
            if file_ext in ['.txt', '.md', '.log']:
                # çº¯æ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content[:5000]  # é™åˆ¶é•¿åº¦ï¼Œé¿å…è¿‡é•¿
            
            elif file_ext in ['.pdf']:
                # PDFæ–‡ä»¶ï¼ˆéœ€è¦å®‰è£…pdfplumberæˆ–ç±»ä¼¼åº“ï¼‰
                try:
                    import pdfplumber
                    with pdfplumber.open(file_path) as pdf:
                        text = ""
                        for page in pdf.pages[:10]:  # é™åˆ¶é¡µæ•°
                            text += page.extract_text() or ""
                    return text[:5000]
                except ImportError:
                    return f"PDFæ–‡æ¡£éœ€è¦å®‰è£…pdfplumberåº“æ‰èƒ½è¯»å–: {file_name}"
                except Exception as e:
                    return f"æ— æ³•è¯»å–PDFæ–‡æ¡£: {file_name}, é”™è¯¯: {str(e)}"
            
            elif file_ext in ['.docx']:
                # Wordæ–‡æ¡£ï¼ˆéœ€è¦å®‰è£…python-docxï¼‰
                try:
                    from docx import Document
                    doc = Document(file_path)
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text[:5000]
                except ImportError:
                    return f"Wordæ–‡æ¡£éœ€è¦å®‰è£…python-docxåº“æ‰èƒ½è¯»å–: {file_name}"
                except Exception as e:
                    return f"æ— æ³•è¯»å–Wordæ–‡æ¡£: {file_name}, é”™è¯¯: {str(e)}"
            
            else:
                return f"æš‚ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {file_name} ({file_ext})"
                
        except Exception as e:
            return f"è¯»å–æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _analyze_with_openrouter_deepseek(self, context_data: Dict[str, Any]) -> List[TaskItem]:
        """ä½¿ç”¨DeepSeek APIè¿›è¡Œæ·±åº¦é¡¹ç›®åˆ†æ"""
        print("ğŸ¤– ä½¿ç”¨DeepSeek APIè¿›è¡Œé¡¹ç›®åˆ†æ...")
        
        # æ„å»ºAIåˆ†æprompt
        prompt = self._build_comprehensive_analysis_prompt_with_documents(context_data)
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system", 
                    "content": """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„é¡¹ç›®ç®¡ç†ä¸“å®¶å’Œå¼€å‘å›¢é˜Ÿé¡¾é—®ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå¼€å‘å›¢é˜Ÿçš„ç¾¤èŠè®°å½•ï¼Œä»ä¸­æå–å‡ºçœŸæ­£çš„é¡¹ç›®ä»»åŠ¡ã€å·²å®Œæˆçš„å·¥ä½œå’Œé‡åˆ°çš„é—®é¢˜ã€‚

ä½ éœ€è¦ï¼š
1. ä»”ç»†é˜…è¯»æ¯ä¸€æ¡æ¶ˆæ¯ï¼Œå‡†ç¡®ç†è§£ä¸Šä¸‹æ–‡
2. è¯†åˆ«å‡ºæ˜ç¡®æåˆ°çš„å¾…åŠä»»åŠ¡(ToDo)ã€å·²å®Œæˆå·¥ä½œ(Done)å’Œé‡åˆ°çš„é—®é¢˜(Issue)
3. æ­£ç¡®å½’å±æ¯ä¸ªä»»åŠ¡åˆ°å…·ä½“çš„äººå‘˜
4. ä¸¥æ ¼åŸºäºå¯¹è¯åŸæ–‡ï¼Œä¸è¦æ¨ç†æˆ–æ·»åŠ ä»»ä½•å†…å®¹
5. æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼å‡†ç¡®è¾“å‡º

é‡è¦ï¼šåªæå–å¯¹è¯ä¸­æ˜ç¡®æåˆ°çš„å†…å®¹ï¼Œä¿æŒåŸæ–‡æè¿°ï¼Œä¸è¦æ¦‚æ‹¬æˆ–é‡å†™ã€‚"""
                },
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥æé«˜å‡†ç¡®æ€§
            "max_tokens": 2000
        }
        
        response = requests.post(self.api_url, headers=headers, json=payload)
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            print("âœ… DeepSeek APIåˆ†æå®Œæˆ")
            
            # è§£æAIå“åº”ï¼Œä½†åŒæ—¶ä¿å­˜åŸå§‹åˆ†ç»„æ•°æ®
            tasks = self._parse_ai_analysis(ai_response, context_data)
            
            # å¦‚æœæˆåŠŸè§£æåˆ°åˆ†ç»„æ•°æ®ï¼Œç›´æ¥è¿”å›å®ƒ
            if hasattr(self, '_parsed_grouped_data') and self._parsed_grouped_data:
                print("ğŸ“‹ ä½¿ç”¨AIç›´æ¥è¿”å›çš„åˆ†ç»„æ•°æ®")
                return self._parsed_grouped_data  # ç›´æ¥è¿”å›åˆ†ç»„æ•°æ®è€Œä¸æ˜¯TaskItemåˆ—è¡¨
            else:
                print("âš ï¸ æœªèƒ½è·å–åˆ°åˆ†ç»„æ•°æ®ï¼Œä½¿ç”¨TaskItemæ ¼å¼")
                return tasks
        else:
            print(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}")
    
    def _build_comprehensive_analysis_prompt_with_documents(self, context_data: Dict[str, Any]) -> str:
        """æ„å»ºåˆ†æpromptï¼Œä¸“æ³¨äºæ–‡æœ¬æ¶ˆæ¯åˆ†æ"""
        participants = context_data["participants"]
        conversation = context_data["conversation_flow"]
        
        # æ„å»ºå‚ä¸è€…åˆ—è¡¨
        participants_text = "é¡¹ç›®å›¢é˜Ÿæˆå‘˜ï¼š\n"
        for p_id, p_info in participants.items():
            participants_text += f"- {p_info['name']} (ID: {p_id}, æ¶ˆæ¯æ•°: {p_info['message_count']})\n"
        
        # æ„å»ºå®Œæ•´å¯¹è¯å†…å®¹
        conversation_text = "\n## å®Œæ•´ç¾¤èŠå¯¹è¯è®°å½•ï¼š\n"
        for i, msg in enumerate(conversation):
            sender_name = msg["sender"]["name"]
            content = msg["content"]
            msg_type = msg.get("type", "text")
            
            # å®‰å…¨åœ°å¤„ç†æ—¶é—´æˆ³è½¬æ¢
            try:
                timestamp = datetime.fromtimestamp(int(msg["timestamp"])).strftime("%m-%d %H:%M")
            except (ValueError, OSError, OverflowError):
                # å¦‚æœæ—¶é—´æˆ³æœ‰é—®é¢˜ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºæ ‡è¯†
                timestamp = f"æ¶ˆæ¯{i+1}"
            
            if msg_type == "file_reference":
                conversation_text += f"[{timestamp}] {sender_name} åˆ†äº«äº†æ–‡æ¡£: {content}\n"
            else:
                conversation_text += f"[{timestamp}] {sender_name}: {content}\n"
            
            # å¦‚æœæœ‰@æåˆ°çš„äººï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
            if msg.get("mentions"):
                mentions = [m.get("name", m.get("id", "")) for m in msg["mentions"]]
                conversation_text += f"    (æåˆ°: {', '.join(mentions)})\n"
        
        # æ—¶é—´èŒƒå›´ - å®‰å…¨å¤„ç†
        timespan = context_data["timespan"]
        try:
            start_time = datetime.fromtimestamp(int(timespan["start"])).strftime("%Y-%m-%d %H:%M:%S")
            end_time = datetime.fromtimestamp(int(timespan["end"])).strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, OSError, OverflowError):
            start_time = "å¼€å§‹æ—¶é—´"
            end_time = "ç»“æŸæ—¶é—´"
        
        # æ„å»ºå›¢é˜Ÿæˆå‘˜åˆ—è¡¨ç”¨äºJSONæ ¼å¼
        member_names = [p_info['name'] for p_info in participants.values()]
        
        prompt = f"""è¯·æ·±åº¦åˆ†æä»¥ä¸‹å¼€å‘å›¢é˜Ÿçš„é¡¹ç›®ç¾¤èŠå¯¹è¯ï¼Œæå–å‡ºçœŸæ­£çš„ä»»åŠ¡ä¿¡æ¯å¹¶æŒ‰äººå‘˜ç»„ç»‡ï¼š

## åŸºæœ¬ä¿¡æ¯
- æ—¶é—´èŒƒå›´: {start_time} è‡³ {end_time}
- æ¶ˆæ¯æ€»æ•°: {context_data["total_messages"]}æ¡
- å‚ä¸äººæ•°: {len(participants)}äºº

{participants_text}

{conversation_text}

## åˆ†æä»»åŠ¡

è¯·ä»å¯¹è¯ä¸­å‡†ç¡®è¯†åˆ«ä»¥ä¸‹ä¸‰ç±»ä¿¡æ¯ï¼š

### 1. ToDo (å¾…åŠä»»åŠ¡)
- æ˜ç¡®æåˆ°éœ€è¦å®Œæˆçš„å·¥ä½œã€å¼€å‘ä»»åŠ¡ã€åŠŸèƒ½éœ€æ±‚
- è¢«åˆ†é…ç»™ç‰¹å®šäººå‘˜çš„ä»»åŠ¡
- è®¡åˆ’è¦åšçš„äº‹æƒ…
- å…³é”®è¯ï¼šéœ€è¦ã€è¦åšã€è´Ÿè´£ã€å¼€å‘ã€å®ç°ã€å®Œæˆã€è®¾è®¡ç­‰

### 2. Done (å·²å®Œæˆ)
- æ˜ç¡®æåˆ°å·²ç»å®Œæˆçš„å·¥ä½œã€è§£å†³çš„é—®é¢˜
- å·²ç»äº¤ä»˜çš„åŠŸèƒ½ã€ä¿®å¤çš„bug
- å–å¾—çš„è¿›å±•å’Œæˆæœ
- å…³é”®è¯ï¼šå®Œæˆäº†ã€å·²ç»ã€æå®šã€è§£å†³äº†ã€ä¸Šçº¿äº†ã€ä¿®å¤äº†ç­‰

### 3. Issue (é‡åˆ°çš„é—®é¢˜)
- é‡åˆ°çš„æŠ€æœ¯é—®é¢˜ã€å›°éš¾ã€é˜»å¡
- éœ€è¦è§£å†³çš„bugã€æ•…éšœ
- å¼€å‘è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜
- å…³é”®è¯ï¼šé—®é¢˜ã€bugã€å‡ºé”™ã€å¼‚å¸¸ã€å›°éš¾ã€é˜»å¡ç­‰

## é‡è¦åŸåˆ™

1. **ä¸¥æ ¼åŸºäºåŸæ–‡**ï¼šåªæå–å¯¹è¯ä¸­æ˜ç¡®æåˆ°çš„å†…å®¹ï¼Œä¸è¦æ¨ç†æˆ–æ·»åŠ 
2. **å‡†ç¡®å½’å±**ï¼šç¡®ä¿ä»»åŠ¡åˆ†é…ç»™æ­£ç¡®çš„äººå‘˜
3. **åˆ†ç±»å‡†ç¡®**ï¼šåŒºåˆ†æ¸…æ¥šå¾…åŠã€å·²å®Œæˆå’Œé—®é¢˜
4. **å†…å®¹å…·ä½“**ï¼šä¿ç•™ä»»åŠ¡çš„å…·ä½“æè¿°ï¼Œä¸è¦è¿‡åº¦æ¦‚æ‹¬

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ‰€æœ‰å›¢é˜Ÿæˆå‘˜éƒ½åŒ…å«åœ¨å†…ï¼š

```json
{{
  "ToDo": {{
    {', '.join([f'"{name}": []' for name in member_names])}
  }},
  "Done": {{
    {', '.join([f'"{name}": []' for name in member_names])}
  }},
  "Issue": {{
    {', '.join([f'"{name}": []' for name in member_names])}
  }}
}}
```

æ¯ä¸ªäººå‘˜çš„ä»»åŠ¡æ•°ç»„åº”è¯¥åŒ…å«å…·ä½“çš„ä»»åŠ¡æè¿°å­—ç¬¦ä¸²ã€‚å¦‚æœå¯¹è¯ä¸­æ²¡æœ‰æ˜ç¡®çš„ä»»åŠ¡ï¼Œå¯¹åº”æ•°ç»„ä¿æŒä¸ºç©ºã€‚

## ç¤ºä¾‹è¾“å‡ºæ ¼å¼

```json
{{
  "ToDo": {{
    "å¼ ä¸‰": ["å®Œæˆç”¨æˆ·ç™»å½•åŠŸèƒ½", "ä¿®å¤æ•°æ®åº“è¿æ¥é—®é¢˜"],
    "æå››": ["è®¾è®¡å‰ç«¯ç•Œé¢", "å‡†å¤‡é¡¹ç›®æ–‡æ¡£"],
    "ç‹äº”": []
  }},
  "Done": {{
    "å¼ ä¸‰": ["å®Œæˆäº†APIæ¥å£å¼€å‘"],
    "æå››": ["ä¸Šçº¿äº†ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½"],
    "ç‹äº”": []
  }},
  "Issue": {{
    "å¼ ä¸‰": ["æ•°æ®åº“è¿æ¥è¶…æ—¶é—®é¢˜"],
    "æå››": [],
    "ç‹äº”": ["å‰ç«¯æ‰“åŒ…å‡ºç°é”™è¯¯"]
  }}
}}
```

è¯·ä»”ç»†åˆ†æå¯¹è¯å†…å®¹ï¼Œå‡†ç¡®æå–å¹¶åˆ†ç±»ä»»åŠ¡ä¿¡æ¯ã€‚
"""
        return prompt
    
    def _parse_ai_analysis(self, ai_response: str, context_data: Dict) -> List[TaskItem]:
        """è§£æAIåˆ†æç»“æœ - ç°åœ¨AIç›´æ¥è¿”å›åˆ†ç»„æ ¼å¼"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_response, re.DOTALL)
            if json_match:
                ai_data = json.loads(json_match.group(1))
            else:
                # å°è¯•ç›´æ¥è§£æ
                json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
                if json_match:
                    ai_data = json.loads(json_match.group())
                else:
                    ai_data = json.loads(ai_response)
            
            # ç°åœ¨AIç›´æ¥è¿”å›åˆ†ç»„æ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦å­˜å‚¨è¿™ä¸ªæ ¼å¼è€Œä¸æ˜¯è½¬æ¢ä¸ºTaskItemåˆ—è¡¨
            # ç›´æ¥è¿”å›åŸå§‹åˆ†ç»„æ•°æ®ï¼Œåç»­å¤„ç†ä¼šä½¿ç”¨è¿™ä¸ªæ ¼å¼
            self._parsed_grouped_data = ai_data
            
            # ä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç æµç¨‹ï¼Œä»ç„¶ç”ŸæˆTaskItemåˆ—è¡¨
            tasks = []
            task_id = 1
            
            # å¤„ç†ToDoä»»åŠ¡
            if 'ToDo' in ai_data:
                for person, task_list in ai_data['ToDo'].items():
                    for task_desc in task_list:
                        task = TaskItem(
                            id=f"todo_{task_id}",
                            title=task_desc,
                            description=task_desc,
                            assignee=person,
                            priority="MEDIUM",
                            status="TODO",
                            tags=[],
                            deadline=None,
                            related_messages=[],
                            confidence=0.8
                        )
                        tasks.append(task)
                        task_id += 1
            
            # å¤„ç†Doneä»»åŠ¡
            if 'Done' in ai_data:
                for person, task_list in ai_data['Done'].items():
                    for task_desc in task_list:
                        task = TaskItem(
                            id=f"done_{task_id}",
                            title=task_desc,
                            description=task_desc,
                            assignee=person,
                            priority="MEDIUM",
                            status="DONE",
                            tags=[],
                            deadline=None,
                            related_messages=[],
                            confidence=0.8
                        )
                        tasks.append(task)
                        task_id += 1
            
            # å¤„ç†Issueé—®é¢˜
            if 'Issue' in ai_data:
                for person, task_list in ai_data['Issue'].items():
                    for task_desc in task_list:
                        task = TaskItem(
                            id=f"issue_{task_id}",
                            title=task_desc,
                            description=task_desc,
                            assignee=person,
                            priority="HIGH",
                            status="ISSUE",
                            tags=[],
                            deadline=None,
                            related_messages=[],
                            confidence=0.8
                        )
                        tasks.append(task)
                        task_id += 1
            
            return tasks
            
        except Exception as e:
            print(f"âš ï¸ è§£æAIåˆ†æç»“æœå¤±è´¥: {e}")
            print(f"âš ï¸ AIå“åº”å†…å®¹: {ai_response[:500]}...")
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
    
    def _extract_person_name(self, sender_info: Dict) -> str:
        """æå–äººå‘˜å§“å"""
        sender_id = sender_info.get("id", "")
        if sender_id:
            # ä½¿ç”¨é£ä¹¦ç”¨æˆ·IDæ˜ å°„è·å–çœŸå®å§“å
            real_name = get_user_name_by_feishu_id(sender_id)
            if not real_name.startswith("ç”¨æˆ·") and real_name != "æœªçŸ¥ç”¨æˆ·":
                return real_name
        
        # å¦‚æœæ˜ å°„å¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„é€»è¾‘
        return sender_info.get("name", sender_info.get("id", "æœªçŸ¥ç”¨æˆ·"))
    
    def _organize_by_person(self, tasks: List[TaskItem], original_data: Dict) -> Dict[str, Any]:
        """æŒ‰äººå‘˜ç»„ç»‡ä»»åŠ¡"""
        # æŒ‰äººå‘˜åˆ†ç»„
        person_tasks = {}
        
        for task in tasks:
            assignee = task.assignee
            if assignee not in person_tasks:
                person_tasks[assignee] = {
                    "person_name": assignee,
                    "todos": [],
                    "dones": [],
                    "issues": []
                }
            
            task_dict = {
                "id": task.id,
                "title": task.title,
                "description": task.description,
                "priority": task.priority,
                "tags": task.tags,
                "deadline": task.deadline,
                "related_messages": task.related_messages,
                "confidence": task.confidence
            }
            
            if task.status == "TODO":
                person_tasks[assignee]["todos"].append(task_dict)
            elif task.status == "DONE":
                person_tasks[assignee]["dones"].append(task_dict)
            elif task.status == "ISSUE":
                person_tasks[assignee]["issues"].append(task_dict)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_todos = sum(len(p["todos"]) for p in person_tasks.values())
        total_dones = sum(len(p["dones"]) for p in person_tasks.values())
        total_issues = sum(len(p["issues"]) for p in person_tasks.values())
        
        return {
            "success": True,
            "summary": {
                "analysis_type": "project_context_analysis",
                "original_data": {
                    "total_messages": original_data.get("total_count", 0),
                    "time_range": original_data.get("time_range", {})
                },
                "task_statistics": {
                    "total_tasks": len(tasks),
                    "total_todos": total_todos,
                    "total_dones": total_dones,
                    "total_issues": total_issues,
                    "total_assignees": len(person_tasks),
                    "high_priority_tasks": sum(1 for t in tasks if t.priority == "HIGH")
                },
                "analyzed_at": datetime.now().isoformat()
            },
            "team_tasks": person_tasks,
            "metadata": {
                "analyzer_version": "2.0.0",
                "ai_model": self.model,
                "processing_mode": "AI" if self.api_key else "Enhanced Rules"
            }
        }
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            "success": False,
            "message": "æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ¶ˆæ¯å†…å®¹",
            "team_tasks": {},
            "summary": {
                "task_statistics": {
                    "total_tasks": 0,
                    "total_assignees": 0
                }
            }
        }
    
    def save_analysis_result(self, result: Dict[str, Any], output_file: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"project_analysis_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=4)
        
        print(f"ğŸ“„ é¡¹ç›®åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file


# ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„ç±»åä½œä¸ºåˆ«å
AIMessageProcessor = AIProjectAnalyzer 